{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.special\n",
    "from sklearn.linear_model import LassoCV, LogisticRegressionCV, LinearRegression, Lasso, LogisticRegression\n",
    "from sklearn.base import clone\n",
    "import joblib\n",
    "from statsmodels.api import OLS\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import pandas as pd\n",
    "import math\n",
    "import shap\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automl import reg_fn_gen, model_y_fn_gen, model_t_fn_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_flaml = True\n",
    "semi_synth = False\n",
    "simple_synth = False\n",
    "max_depth = 3\n",
    "stack = False\n",
    "n_splits = 5\n",
    "data = 'welfare'\n",
    "plot = True\n",
    "scale = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_flaml:\n",
    "    from myflaml import auto_reg, auto_clf, auto_weighted_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The moment function, whose average we want to estimate: E[m(X; g)]\n",
    "def moment_fn(x, test_fn):\n",
    "    t1 = np.hstack([np.ones((x.shape[0], 1)), x[:, 1:]])\n",
    "    t0 = np.hstack([np.zeros((x.shape[0], 1)), x[:, 1:]])\n",
    "    return test_fn(t1) - test_fn(t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pehe(cate, preds):\n",
    "    return np.sqrt(np.mean((cate - preds)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and manipulation\n",
    "\n",
    "if data == '401k':\n",
    "    abtest = False\n",
    "    def get_data(q=1, synthetic=False, random_seed=123, true_f=None, gen_epsilon=None):\n",
    "        dfx = pd.read_csv('./401k/401k-x.csv', sep=',')\n",
    "        dfy = pd.read_csv('./401k/401k-y.csv', sep=',')\n",
    "        dfd = pd.read_csv('./401k/401k-d.csv', sep=',')\n",
    "        dfx = dfx.drop(['db'], axis=1) # highly predictable from income\n",
    "        X = np.hstack([dfd.values, dfx.values])\n",
    "        column_names = ['treatment'] + list(dfx.columns)\n",
    "        y = dfy.values.flatten() # outcome to numpy array\n",
    "\n",
    "        mask = (dfx['inc'] > 0) & (dfx['inc'] >= np.percentile(dfx['inc'], 1))\n",
    "        mask &= (dfx['inc'] <= np.percentile(dfx['inc'], 99))\n",
    "        X = X[mask]\n",
    "        y = y[mask]\n",
    "        inds = np.arange(X.shape[0])\n",
    "        np.random.shuffle(inds)\n",
    "        X, y = X[inds], y[inds]\n",
    "\n",
    "        if synthetic: # impute synthetic outcome based on known relationship  \n",
    "            y = true_f(X) + gen_epsilon(X.shape[0])\n",
    "            print(np.mean(moment_fn(X, true_f)))\n",
    "\n",
    "        return X, y, [1, 2, 3, 4], column_names\n",
    "elif data == 'criteo':\n",
    "    abtest = True\n",
    "    def get_data():\n",
    "        df = pd.read_csv('./criteo-uplift-v2.1.csv')\n",
    "        y = df['visit'].values\n",
    "        T = df[['treatment']].values\n",
    "        df = df.drop(['treatment', 'conversion', 'visit', 'exposure'], axis=1)\n",
    "        column_names = ['treatment'] + list(df.columns)\n",
    "        X = np.hstack([T, df.values])\n",
    "        return X, y, np.arange(1, X.shape[1]), column_names\n",
    "elif data == 'welfare':\n",
    "    abtest = True\n",
    "    def get_data():\n",
    "        df = pd.read_csv('./welfarenolabel3.csv', na_values=-999)\n",
    "        continuous = ['hrs1', 'income', 'rincome', 'age', 'polviews',\n",
    "                      'educ', 'earnrs', 'sibs', 'childs', 'occ80', 'prestg80', 'indus80',\n",
    "                      'res16', 'reg16', 'family16', 'parborn', 'maeduc', 'degree', \n",
    "                      'hompop', 'babies', 'preteen', 'teens', 'adults']\n",
    "        categorical = ['partyid', 'wrkstat', 'wrkslf', 'marital', 'race', 'mobile16', 'sex', 'born']\n",
    "        df = df[['y', 'w'] + continuous + categorical]\n",
    "        df = df.dropna()\n",
    "        df = df[~((df['polviews']>4) & (df['polviews'] < 5))]\n",
    "        df = pd.get_dummies(df, columns=categorical, drop_first=True)\n",
    "        display(df.describe())\n",
    "        y = df['y'].values\n",
    "        T = df[['w']].values\n",
    "        df = df.drop(['y', 'w'], axis=1)\n",
    "        column_names = ['treatment'] + list(df.columns)\n",
    "        X = np.hstack([T, df.values])\n",
    "        return X, y, np.arange(1, len(continuous) + 1), column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_f(X):\n",
    "    return np.zeros(X.shape[0])\n",
    "\n",
    "X, y, num_cols, column_names = get_data()\n",
    "feat_ind = 2\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for semi-synthetic data generation\n",
    "if semi_synth:\n",
    "    if simple_synth:\n",
    "        def true_f(X):\n",
    "            y = (-.7 - .4 * scipy.special.expit(3 * (1 - 2*(X[:, 2] - 30000)))) * X[:, 0]\n",
    "            y += scipy.special.expit(3 * (1 - 2 * (X[:, 2] > 30000)))\n",
    "            return y\n",
    "\n",
    "#         def true_f(X):\n",
    "#             y = (np.log(1 + X[:, 2])) * X[:, 0]\n",
    "#             y += scipy.special.expit(3 * (1 - 2 * (X[:, 2] > 30000)))\n",
    "#             return y\n",
    "\n",
    "#         def true_f(X):\n",
    "#             y = -.7 * X[:, 0]\n",
    "#             y += scipy.special.expit(3 * (1 - 2 * (X[:, 2] > 30000)))\n",
    "#             return y\n",
    "\n",
    "        def gen_epsilon(n):\n",
    "            scale = np.std(true_f(X))   \n",
    "            return np.random.normal(0, .8 * scale, size=n)\n",
    "    else:\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "        fitted_true_model = RandomForestRegressor(min_samples_leaf=50, max_depth=max_depth).fit(X, y)\n",
    "\n",
    "        explainer = shap.KernelExplainer(lambda X: moment_fn(X, fitted_true_model.predict),\n",
    "                                         X[np.random.choice(X.shape[0], 100)])\n",
    "        shap_values = explainer.shap_values(X[:100], nsamples=100)\n",
    "        shap.summary_plot(shap_values, feature_names=column_names)\n",
    "\n",
    "        def true_f(X):\n",
    "            return fitted_true_model.predict(X)\n",
    "\n",
    "        true_residuals = y - cross_val_predict(clone(fitted_true_model), X, y, cv=5)\n",
    "        def gen_epsilon(n):\n",
    "            return scale * np.random.choice(true_residuals, size=n)\n",
    "\n",
    "    X, y, num_cols, column_names = get_data(synthetic=True, true_f=true_f, gen_epsilon=gen_epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, Xval, y, yval = train_test_split(X, y, train_size=.6, shuffle=True)\n",
    "Xval, Xtest, yval, ytest = train_test_split(Xval, yval, train_size=.5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuisance Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=123)\n",
    "splits = list(cv.split(X, X[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_flaml:\n",
    "    model_reg = auto_reg(X, y, n_splits=n_splits, verbose=3, time_budget=60)\n",
    "    model_y = auto_reg(X[:, 1:], y, n_splits=n_splits, verbose=3, time_budget=60)\n",
    "    model_t = auto_clf(X[:, 1:], X[:, 0], n_splits=n_splits, verbose=3)\n",
    "    model_reg_zero = auto_reg(X[X[:, 0]==0, 1:], y[X[:, 0]==0], n_splits=n_splits, verbose=3, time_budget=60)\n",
    "    model_reg_one = auto_reg(X[X[:, 0]==1, 1:], y[X[:, 0]==1], n_splits=n_splits, verbose=3, time_budget=60)\n",
    "else:\n",
    "    model_reg = reg_fn_gen(num_cols, cv=splits, stack=stack, verbose=3)(X, y)\n",
    "    model_y = model_y_fn_gen([t - 1 for t in num_cols[1:]], cv=splits, stack=stack)(X[:, 1:], y)\n",
    "    model_t = model_t_fn_gen([t - 1 for t in num_cols[1:]], cv=splits, stack=stack)(X[:, 1:], X[:, 0])\n",
    "    model_reg_zero = reg_fn_gen([t - 1 for t in num_cols[1:]], cv=5, stack=stack)(X[X[:, 0]==0, 1:], y[X[:, 0]==0])\n",
    "    model_reg_one = reg_fn_gen([t - 1 for t in num_cols[1:]], cv=5, stack=stack)(X[X[:, 0]==1, 1:], y[X[:, 0]==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump([model_reg(), model_y(), model_t(), model_reg_zero(), model_reg_one()], 'nuisance.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mreg, my, mt, mreg_zero, mreg_one = joblib.load('nuisance.jbl')\n",
    "# model_reg = lambda: clone(mreg)\n",
    "# model_y = lambda: clone(my)\n",
    "# model_t = lambda: clone(mt)\n",
    "# model_reg_zero = lambda: clone(mreg_zero)\n",
    "# model_reg_one = lambda: clone(mreg_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "score_reg = np.mean(cross_val_score(model_reg(), X, y, cv=n_splits, scoring='r2'))\n",
    "print(f'model_reg: {score_reg:.3f}')\n",
    "score_reg = np.mean(cross_val_score(model_y(), X[:, 1:], y, cv=n_splits, scoring='r2'))\n",
    "print(f'model_y: {score_reg:.3f}')\n",
    "score_reg = np.mean(cross_val_score(model_t(), X[:, 1:], X[:, 0], cv=n_splits, scoring='r2'))\n",
    "print(f'model_t: {score_reg:.3f}')\n",
    "score_reg = np.mean(cross_val_score(model_reg_zero(), X[X[:, 0]==0, 1:], y[X[:, 0]==0], cv=n_splits, scoring='r2'))\n",
    "print(f'model_reg_zero: {score_reg:.3f}')\n",
    "score_reg = np.mean(cross_val_score(model_reg_one(), X[X[:, 0]==1, 1:], y[X[:, 0]==1], cv=n_splits, scoring='r2'))\n",
    "print(f'model_reg_one: {score_reg:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuisance Cross-Fitted Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "\n",
    "splits = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=123).split(X, X[:, 0]))\n",
    "\n",
    "n = X.shape[0]\n",
    "reg_preds = np.zeros(n)\n",
    "reg_zero_preds = np.zeros(n)\n",
    "reg_one_preds = np.zeros(n)\n",
    "reg_zero_preds_t = np.zeros(n)\n",
    "reg_one_preds_t = np.zeros(n)\n",
    "\n",
    "for train, test in splits:\n",
    "    reg = model_reg().fit(X[train], y[train])\n",
    "    reg_preds[test] = reg.predict(X[test])\n",
    "    reg_one_preds[test] = reg.predict(np.hstack([np.ones((len(test), 1)), X[test, 1:]]))\n",
    "    reg_zero_preds[test] = reg.predict(np.hstack([np.zeros((len(test), 1)), X[test, 1:]]))\n",
    "    \n",
    "    reg_zero = model_reg_zero().fit(X[train][X[train, 0]==0, 1:], y[train][X[train, 0]==0])\n",
    "    reg_one = model_reg_one().fit(X[train][X[train, 0]==1, 1:], y[train][X[train, 0]==1])\n",
    "    reg_zero_preds_t[test] = reg_zero.predict(X[test, 1:])\n",
    "    reg_one_preds_t[test] = reg_one.predict(X[test, 1:])\n",
    "\n",
    "res_preds = cross_val_predict(model_y(), X[:, 1:], y, cv=splits)\n",
    "prop_preds = cross_val_predict(model_t(), X[:, 1:], X[:, 0], cv=splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CATE Model Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hetero_inds = np.arange(1, X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_flaml:\n",
    "    model_final_fn = lambda X, y: auto_reg(X, y, n_splits=n_splits, verbose=3, time_budget=60)\n",
    "else:\n",
    "    model_final_num_cols = [it for it, t in enumerate(hetero_inds) if t in num_cols]\n",
    "    model_final_fn = model_y_fn_gen(model_final_num_cols,\n",
    "                                    cv=KFold(n_splits, shuffle=True, random_state=123),\n",
    "                                    stack=stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slearner\n",
    "slearner_best = model_final_fn(X[:, hetero_inds], reg_one_preds - reg_zero_preds)\n",
    "slearner = slearner_best().fit(X[:, hetero_inds], reg_one_preds - reg_zero_preds)\n",
    "slearner_preds = slearner.predict(X[:, hetero_inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tlearner\n",
    "tlearner_best = model_final_fn(X[:, hetero_inds], reg_one_preds_t - reg_zero_preds_t)\n",
    "tlearner = tlearner_best().fit(X[:, hetero_inds], reg_one_preds_t - reg_zero_preds_t)\n",
    "tlearner_preds = tlearner.predict(X[:, hetero_inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xlearner\n",
    "tau1_preds = y[X[:, 0]==1] - reg_zero_preds_t[X[:, 0]==1]\n",
    "tau0_preds = reg_one_preds_t[X[:, 0]==0] - y[X[:, 0]==0]\n",
    "tau1 = model_final_fn(X[X[:, 0]==1][:, hetero_inds], tau1_preds)().fit(X[X[:, 0]==1][:, hetero_inds], tau1_preds)\n",
    "tau0 = model_final_fn(X[X[:, 0]==0][:, hetero_inds], tau0_preds)().fit(X[X[:, 0]==0][:, hetero_inds], tau0_preds)\n",
    "xtarget = prop_preds * tau1.predict(X[:, hetero_inds]) + (1 - prop_preds) * tau0.predict(X[:, hetero_inds])\n",
    "xlearner = model_final_fn(X[:, hetero_inds], xtarget)().fit(X[:, hetero_inds], xtarget)\n",
    "xlearner_preds = xlearner.predict(X[:, hetero_inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drlearner\n",
    "dr_preds = reg_one_preds - reg_zero_preds\n",
    "dr_preds += (y - reg_preds) * (X[:, 0] - prop_preds) / np.clip(prop_preds * (1 - prop_preds), .09, np.inf)\n",
    "drlearner_best = model_final_fn(X[:, hetero_inds], dr_preds)\n",
    "drlearner = drlearner_best().fit(X[:, hetero_inds], dr_preds)\n",
    "drlearner_preds = drlearner.predict(X[:, hetero_inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rlearner\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from econml.sklearn_extensions.linear_model import WeightedLassoCV\n",
    "\n",
    "yres = y - res_preds\n",
    "tres = X[:, 0] - prop_preds\n",
    "tres = np.clip(tres, .001, np.inf) * (tres >= 0) + np.clip(tres, -np.inf, -.001) * (tres < 0)\n",
    "\n",
    "if use_flaml:\n",
    "    rlearner_fn = auto_weighted_reg(X[:, hetero_inds], yres / tres, sample_weight=tres**2)\n",
    "    rlearner_rf = rlearner_fn().fit(X[:, hetero_inds], yres / tres, sample_weight=tres**2)\n",
    "    rlearner_poly = rlearner_rf\n",
    "else:    \n",
    "    rlearner_fn = lambda: RandomForestRegressor(max_depth=2, min_samples_leaf=100, random_state=123)\n",
    "    rlearner_rf = rlearner_fn().fit(X[:, hetero_inds], yres / tres, sample_weight=tres**2)\n",
    "    rlearner_fn = lambda: Pipeline([('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "                                    ('lr', WeightedLassoCV())])\n",
    "    rlearner_poly = rlearner_fn().fit(X[:, hetero_inds], yres / tres, lr__sample_weight=tres**2)\n",
    "\n",
    "rlearner_preds_rf = rlearner_rf.predict(X[:, hetero_inds])\n",
    "rlearner_preds_poly = rlearner_poly.predict(X[:, hetero_inds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Score Estimation and Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_val = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=123).split(Xval, Xval[:, 0]))\n",
    "\n",
    "n = Xval.shape[0]\n",
    "reg_preds_val = np.zeros(n)\n",
    "reg_zero_preds_val = np.zeros(n)\n",
    "reg_one_preds_val = np.zeros(n)\n",
    "\n",
    "for train, test in splits_val:\n",
    "    reg_val = model_reg().fit(Xval[train], yval[train])\n",
    "    reg_preds_val[test] = reg_val.predict(Xval[test])\n",
    "    reg_one_preds_val[test] = reg_val.predict(np.hstack([np.ones((len(test), 1)), Xval[test, 1:]]))\n",
    "    reg_zero_preds_val[test] = reg_val.predict(np.hstack([np.zeros((len(test), 1)), Xval[test, 1:]]))\n",
    "\n",
    "res_preds_val = cross_val_predict(model_y(), Xval[:, 1:], yval, cv=splits_val)\n",
    "prop_preds_val = cross_val_predict(model_t(), Xval[:, 1:], Xval[:, 0], cv=splits_val)\n",
    "yres_val = yval - res_preds_val\n",
    "tres_val = Xval[:, 0] - prop_preds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_val = reg_one_preds_val - reg_zero_preds_val\n",
    "reisz_val = (Xval[:, 0] - prop_preds_val) / np.clip(prop_preds_val * (1 - prop_preds_val), .09, np.inf)\n",
    "dr_val += (yval - reg_preds_val) * reisz_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_ate_val_r = np.mean(yres_val * tres_val) / np.mean(tres_val**2)\n",
    "def rscore(cate_preds): \n",
    "    rscore_t = np.mean((yres_val - cate_preds * tres_val)**2)\n",
    "    rscore_b = np.mean((yres_val - overall_ate_val_r * tres_val)**2)\n",
    "    return 1 - rscore_t / rscore_b\n",
    "\n",
    "overall_ate_val_dr = np.mean(dr_val)\n",
    "def drscore(cate_preds):\n",
    "    drscore_t = np.mean((dr_val - cate_preds)**2)\n",
    "    drscore_b = np.mean((dr_val - overall_ate_val_dr)**2)\n",
    "    return 1 - drscore_t / drscore_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qscore(cate_preds):\n",
    "    ugrid = np.linspace(1, 99, 50)\n",
    "    qs = np.percentile(cate_preds, ugrid)\n",
    "\n",
    "    ate = np.zeros((len(qs), 3))\n",
    "    true_ate = np.zeros(len(qs))\n",
    "    psi = np.zeros((len(qs), dr_val.shape[0]))\n",
    "    n = len(dr_val)\n",
    "    all_ate = np.mean(dr_val)\n",
    "    for it in range(len(qs)):\n",
    "        inds = (qs[it] <= cate_preds)\n",
    "        prob = np.mean(inds)\n",
    "        psi[it, :] = (dr_val - all_ate) * (inds - prob)\n",
    "        ate[it, 0] = np.mean(psi[it])\n",
    "        psi[it, :] -= ate[it, 0]\n",
    "        ate[it, 1] = np.sqrt(np.mean(psi[it]**2) / n)\n",
    "        ate[it, 2] = prob\n",
    "    qini_psi = np.sum(psi[:-1] * np.diff(ugrid).reshape(-1, 1) / 100, 0)\n",
    "    qini = np.sum(ate[:-1, 0] * np.diff(ugrid) / 100)\n",
    "    qini_stderr = np.sqrt(np.mean(qini_psi**2) / n)\n",
    "    return (qini - qini_stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calscore(cate_preds, cate_preds_train):\n",
    "    ugrid = np.arange(0, 101, 25)\n",
    "    qs = np.percentile(cate_preds_train, ugrid)\n",
    "    qs[-1] = np.inf\n",
    "    qs[0] = -np.inf\n",
    "\n",
    "    all_ate = np.mean(dr_preds)\n",
    "    ate = np.zeros((len(qs) - 1, 3))\n",
    "    for it in range(len(qs) - 1):\n",
    "        inds = (qs[it] <= cate_preds) & (cate_preds < qs[it + 1])\n",
    "        ate[it, 2] = np.mean(inds)\n",
    "        if ate[it, 2] > 0:\n",
    "            ate[it, 0] = np.mean(dr_val[inds])\n",
    "            ate[it, 1] = np.mean(cate_preds[inds])\n",
    "\n",
    "    cal = np.sum(ate[:, 2] * np.abs(ate[:, 0] - ate[:, 1]))\n",
    "    calbase = np.sum(ate[:, 2] * np.abs(ate[:, 0] - all_ate)) + 1e-12\n",
    "    return 1 - cal / calbase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score CATE Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slearner_rscore = rscore(slearner.predict(Xval[:, hetero_inds]))\n",
    "slearner_drscore = drscore(slearner.predict(Xval[:, hetero_inds]))\n",
    "slearner_qscore = qscore(slearner.predict(Xval[:, hetero_inds]))\n",
    "slearner_calscore = calscore(slearner.predict(Xval[:, hetero_inds]), slearner_preds)\n",
    "print(slearner_rscore, slearner_drscore, slearner_qscore, slearner_calscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlearner_rscore = rscore(tlearner.predict(Xval[:, hetero_inds]))\n",
    "tlearner_drscore = drscore(tlearner.predict(Xval[:, hetero_inds]))\n",
    "tlearner_qscore = qscore(tlearner.predict(Xval[:, hetero_inds]))\n",
    "tlearner_calscore = calscore(tlearner.predict(Xval[:, hetero_inds]), tlearner_preds)\n",
    "print(tlearner_rscore, tlearner_drscore, tlearner_qscore, tlearner_calscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlearner_rscore = rscore(xlearner.predict(Xval[:, hetero_inds]))\n",
    "xlearner_drscore = drscore(xlearner.predict(Xval[:, hetero_inds]))\n",
    "xlearner_qscore = qscore(xlearner.predict(Xval[:, hetero_inds]))\n",
    "xlearner_calscore = calscore(xlearner.predict(Xval[:, hetero_inds]), xlearner_preds)\n",
    "print(xlearner_rscore, xlearner_drscore, xlearner_qscore, xlearner_calscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drlearner_rscore = rscore(drlearner.predict(Xval[:, hetero_inds]))\n",
    "drlearner_drscore = drscore(drlearner.predict(Xval[:, hetero_inds]))\n",
    "drlearner_qscore = qscore(drlearner.predict(Xval[:, hetero_inds]))\n",
    "drlearner_calscore = calscore(drlearner.predict(Xval[:, hetero_inds]), drlearner_preds)\n",
    "print(drlearner_rscore, drlearner_drscore, drlearner_qscore, drlearner_calscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlearner_rf_rscore = rscore(rlearner_rf.predict(Xval[:, hetero_inds]))\n",
    "rlearner_rf_drscore = drscore(rlearner_rf.predict(Xval[:, hetero_inds]))\n",
    "rlearner_rf_qscore = qscore(rlearner_rf.predict(Xval[:, hetero_inds]))\n",
    "rlearner_rf_calscore = calscore(rlearner_rf.predict(Xval[:, hetero_inds]), rlearner_preds_rf)\n",
    "print(rlearner_rf_rscore, rlearner_rf_drscore, rlearner_rf_qscore, rlearner_rf_calscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlearner_poly_rscore = rscore(rlearner_poly.predict(Xval[:, hetero_inds]))\n",
    "rlearner_poly_drscore = drscore(rlearner_poly.predict(Xval[:, hetero_inds]))\n",
    "rlearner_poly_qscore = qscore(rlearner_poly.predict(Xval[:, hetero_inds]))\n",
    "rlearner_poly_calscore = calscore(rlearner_poly.predict(Xval[:, hetero_inds]), rlearner_preds_poly)\n",
    "print(rlearner_poly_rscore, rlearner_poly_drscore, rlearner_poly_qscore, rlearner_poly_calscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting CATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot:\n",
    "    plt.figure(figsize=(20, 7))\n",
    "    plt.subplot(1, 5, 1)\n",
    "    plt.scatter(X[:, feat_ind], slearner_preds, label='slearner', alpha=.4)\n",
    "    plt.scatter(X[:, feat_ind], moment_fn(X, true_f), label='True')\n",
    "    plt.title(f'Rscore={slearner_rscore:.3f}, DRscore={slearner_drscore:.3f}\\n '\n",
    "              f'PEHE={pehe(moment_fn(X, true_f), slearner_preds):.5f}')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 5, 2)\n",
    "    plt.scatter(X[:, feat_ind], tlearner_preds, label='tlearner', alpha=.4)\n",
    "    plt.scatter(X[:, feat_ind], moment_fn(X, true_f), label='True')\n",
    "    plt.title(f'Rscore={tlearner_rscore:.3f}, DRscore={tlearner_drscore:.3f}\\n '\n",
    "              f'PEHE={pehe(moment_fn(X, true_f), tlearner_preds):.5f}')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 5, 3)\n",
    "    plt.scatter(X[:, feat_ind], xlearner_preds, label='xlearner', alpha=.4)\n",
    "    plt.scatter(X[:, feat_ind], moment_fn(X, true_f), label='True')\n",
    "    plt.title(f'Rscore={xlearner_rscore:.3f}, DRscore={xlearner_drscore:.3f}\\n '\n",
    "              f'PEHE={pehe(moment_fn(X, true_f), xlearner_preds):.5f}')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 5, 4)\n",
    "    plt.scatter(X[:, feat_ind], drlearner_preds, label='drlearner', alpha=.4)\n",
    "    plt.scatter(X[:, feat_ind], moment_fn(X, true_f), label='True')\n",
    "    plt.title(f'Rscore={drlearner_rscore:.3f}, DRscore={drlearner_drscore:.3f}\\n '\n",
    "              f'PEHE={pehe(moment_fn(X, true_f), drlearner_preds):.5f}')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 5, 5)\n",
    "    plt.scatter(X[:, feat_ind], rlearner_preds_rf, label='rlearner_rf', alpha=.4)\n",
    "    plt.scatter(X[:, feat_ind], rlearner_preds_poly, label='rlearner_poly', alpha=.4)\n",
    "    plt.scatter(X[:, feat_ind], moment_fn(X, true_f), label='True')\n",
    "    plt.title(f'Rscore: rf={rlearner_rf_rscore:.3f}, poly={rlearner_poly_rscore:.3f}\\n '\n",
    "              f'DRscore: rf={rlearner_rf_drscore:.3f}, poly={rlearner_poly_drscore:.3f}\\n '\n",
    "              f'PEHE: rf={pehe(moment_fn(X, true_f), rlearner_preds_rf):.5f}, '\n",
    "              f'poly={pehe(moment_fn(X, true_f), rlearner_preds_poly):.5f}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Model Selection and Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Ensemble(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, models, weights):\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.sum(self.weights.reshape((-1, 1)) * np.array([m.predict(X) for m in self.models]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [slearner, tlearner, xlearner, drlearner, rlearner_rf, rlearner_poly]\n",
    "scores = np.array([slearner_rscore, tlearner_rscore, xlearner_rscore, drlearner_rscore,\n",
    "                   rlearner_rf_rscore, rlearner_poly_rscore])\n",
    "\n",
    "eta_grid = np.logspace(-3, 4, 5)\n",
    "ens = {}\n",
    "for eta in eta_grid:\n",
    "    weights = scipy.special.softmax(eta * scores)\n",
    "    ensemble = Ensemble(models, weights)\n",
    "    ens[eta] = (ensemble, rscore(ensemble.predict(Xval[:, hetero_inds])))\n",
    "\n",
    "rscore_best = -np.inf\n",
    "for eta in eta_grid:\n",
    "    if ens[eta][1] >= rscore_best:\n",
    "        rscore_best = ens[eta][1]\n",
    "        eta_best = eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot:\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    for it, eta in enumerate(eta_grid):\n",
    "        plt.subplot(1, len(eta_grid), it + 1)\n",
    "        plt.scatter(X[:, feat_ind], ens[eta][0].predict(X[:, hetero_inds]), label=f'eta={eta:.3f}', alpha=.4)\n",
    "        plt.scatter(X[:, feat_ind], moment_fn(X, true_f), label='True')\n",
    "        plt.title(f'Rscore={ens[eta][1]:.3f}')\n",
    "        plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.scatter(X[:, feat_ind], ens[eta_best][0].predict(X[:, hetero_inds]), label=f'eta={eta_best:.3f}', alpha=.4)\n",
    "    plt.scatter(X[:, feat_ind], moment_fn(X, true_f), label='True')\n",
    "    plt.title(f'Rscore={ens[eta_best][1]:.5f}, '\n",
    "              f'PEHE={pehe(moment_fn(X, true_f), ens[eta_best][0].predict(X[:, hetero_inds])):.5f}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rscore_best = ens[eta_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [slearner, tlearner, xlearner, drlearner, rlearner_rf, rlearner_poly]\n",
    "scores = np.array([slearner_drscore, tlearner_drscore, xlearner_drscore, drlearner_drscore,\n",
    "                   rlearner_rf_drscore, rlearner_poly_drscore])\n",
    "\n",
    "eta_grid = np.logspace(-3, 4, 5)\n",
    "ens = {}\n",
    "for eta in eta_grid:\n",
    "    weights = scipy.special.softmax(eta * scores)\n",
    "    ensemble = Ensemble(models, weights)\n",
    "    ens[eta] = (ensemble, drscore(ensemble.predict(Xval[:, hetero_inds])))\n",
    "\n",
    "drscore_best = -np.inf\n",
    "for eta in eta_grid:\n",
    "    if ens[eta][1] >= drscore_best:\n",
    "        drscore_best = ens[eta][1]\n",
    "        eta_best = eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot:\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    for it, eta in enumerate(eta_grid):\n",
    "        plt.subplot(1, len(eta_grid), it + 1)\n",
    "        plt.scatter(X[:, feat_ind], ens[eta][0].predict(X[:, hetero_inds]), label=f'eta={eta:.3f}', alpha=.4)\n",
    "        plt.scatter(X[:, feat_ind], moment_fn(X, true_f), label='True')\n",
    "        plt.title(f'DRscore={ens[eta][1]:.3f}')\n",
    "        plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.scatter(X[:, feat_ind], ens[eta_best][0].predict(X[:, hetero_inds]), label=f'eta={eta_best:.3f}', alpha=.4)\n",
    "    plt.scatter(X[:, feat_ind], moment_fn(X, true_f), label='True', alpha=.3)\n",
    "    plt.title(f'DRscore={ens[eta_best][1]:.5f}, '\n",
    "              f'PEHE={pehe(moment_fn(X, true_f), ens[eta_best][0].predict(X[:, hetero_inds])):.5f}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drscore_best = ens[eta_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [slearner, tlearner, xlearner, drlearner, rlearner_rf, rlearner_poly]\n",
    "scores = np.array([slearner_qscore, tlearner_qscore, xlearner_qscore, drlearner_qscore,\n",
    "                   rlearner_rf_qscore, rlearner_poly_qscore])\n",
    "\n",
    "eta_grid = np.logspace(-3, 4, 5)\n",
    "ens = {}\n",
    "for eta in eta_grid:\n",
    "    weights = scipy.special.softmax(eta * scores)\n",
    "    ensemble = Ensemble(models, weights)\n",
    "    ens[eta] = (ensemble, qscore(ensemble.predict(Xval[:, hetero_inds])))\n",
    "\n",
    "qscore_best = -np.inf\n",
    "for eta in eta_grid:\n",
    "    if ens[eta][1] >= qscore_best:\n",
    "        qscore_best = ens[eta][1]\n",
    "        eta_best = eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot:\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    for it, eta in enumerate(eta_grid):\n",
    "        plt.subplot(1, len(eta_grid), it + 1)\n",
    "        plt.scatter(X[:, feat_ind], ens[eta][0].predict(X[:, hetero_inds]), label=f'eta={eta:.3f}', alpha=.4)\n",
    "        plt.scatter(X[:, feat_ind], moment_fn(X, true_f), label='True')\n",
    "        plt.title(f'Qscore={ens[eta][1]:.3f}')\n",
    "        plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.scatter(X[:, feat_ind], ens[eta_best][0].predict(X[:, hetero_inds]), label=f'eta={eta_best:.3f}', alpha=.4)\n",
    "    plt.scatter(X[:, feat_ind], moment_fn(X, true_f), label='True', alpha=.3)\n",
    "    plt.title(f'Qscore={ens[eta_best][1]:.5f}, '\n",
    "              f'PEHE={pehe(moment_fn(X, true_f), ens[eta_best][0].predict(X[:, hetero_inds])):.5f}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qscore_best = ens[eta_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [slearner, tlearner, xlearner, drlearner, rlearner_rf, rlearner_poly]\n",
    "scores = np.array([slearner_calscore, tlearner_calscore, xlearner_calscore, drlearner_calscore,\n",
    "                   rlearner_rf_calscore, rlearner_poly_calscore])\n",
    "\n",
    "eta_grid = np.logspace(-3, 4, 5)\n",
    "ens = {}\n",
    "for eta in eta_grid:\n",
    "    weights = scipy.special.softmax(eta * scores)\n",
    "    ensemble = Ensemble(models, weights)\n",
    "    ens[eta] = (ensemble, calscore(ensemble.predict(Xval[:, hetero_inds]),\n",
    "                                   ensemble.predict(X[:, hetero_inds])))\n",
    "\n",
    "calscore_best = -np.inf\n",
    "for eta in eta_grid:\n",
    "    if ens[eta][1] >= calscore_best:\n",
    "        calscore_best = ens[eta][1]\n",
    "        eta_best = eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot:\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    for it, eta in enumerate(eta_grid):\n",
    "        plt.subplot(1, len(eta_grid), it + 1)\n",
    "        plt.scatter(X[:, feat_ind], ens[eta][0].predict(X[:, hetero_inds]), label=f'eta={eta:.3f}', alpha=.4)\n",
    "        plt.scatter(X[:, feat_ind], moment_fn(X, true_f), label='True')\n",
    "        plt.title(f'Calscore={ens[eta][1]:.3f}')\n",
    "        plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.scatter(X[:, feat_ind], ens[eta_best][0].predict(X[:, hetero_inds]), label=f'eta={eta_best:.3f}', alpha=.4)\n",
    "    plt.scatter(X[:, feat_ind], moment_fn(X, true_f), label='True', alpha=.3)\n",
    "    plt.title(f'Calscore={ens[eta_best][1]:.5f}, '\n",
    "              f'PEHE={pehe(moment_fn(X, true_f), ens[eta_best][0].predict(X[:, hetero_inds])):.5f}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calscore_best = ens[eta_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_best = Ensemble([rscore_best[0], drscore_best[0], qscore_best[0], calscore_best[0]],\n",
    "                        np.array([.25, .25, .25, .25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, feat_ind], overall_best.predict(X[:, hetero_inds]), label=f'eta={eta_best:.3f}', alpha=.4)\n",
    "plt.scatter(X[:, feat_ind], moment_fn(X, true_f), label='True', alpha=.3)\n",
    "plt.title(f'PEHE={pehe(moment_fn(X, true_f), overall_best.predict(X[:, hetero_inds])):.5f}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(overall_best, 'ensemble_cate.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall_best = joblib.load('ensemble_cate.jbl')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation of Learned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explainer = shap.KernelExplainer(overall_best.predict, X[np.random.choice(X.shape[0], 100)][:, hetero_inds])\n",
    "# shap_values = explainer.shap_values(Xval[:100, hetero_inds], nsamples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap.summary_plot(shap_values, feature_names=np.array(column_names)[hetero_inds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Based on Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_test = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=123).split(Xtest, Xtest[:, 0]))\n",
    "\n",
    "n = Xtest.shape[0]\n",
    "reg_preds_test = np.zeros(n)\n",
    "reg_one_preds_test = np.zeros(n)\n",
    "reg_zero_preds_test = np.zeros(n)\n",
    "\n",
    "for train, test in splits_test:\n",
    "    reg_test = model_reg().fit(Xtest[train], ytest[train])\n",
    "    reg_preds_test[test] = reg_test.predict(Xtest[test])\n",
    "    reg_one_preds_test[test] = reg_test.predict(np.hstack([np.ones((len(test), 1)), Xtest[test, 1:]]))\n",
    "    reg_zero_preds_test[test] = reg_test.predict(np.hstack([np.zeros((len(test), 1)), Xtest[test, 1:]]))\n",
    "\n",
    "prop_preds_test = cross_val_predict(model_t(), Xtest[:, 1:], Xtest[:, 0], cv=splits_test)\n",
    "res_preds_test = cross_val_predict(model_y(), Xtest[:, 1:], ytest, cv=splits_test)\n",
    "yres_test = ytest - res_preds_test\n",
    "tres_test = Xtest[:, 0] - prop_preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_test = reg_one_preds_test - reg_zero_preds_test\n",
    "reisz_test = (Xtest[:, 0] - prop_preds_test) / np.clip(prop_preds_test * (1 - prop_preds_test), .09, np.inf)\n",
    "dr_test += (ytest - reg_preds_test) * reisz_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_test = overall_best.predict(Xtest[:, hetero_inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = np.percentile(cate_test, np.arange(0, 101, 25))\n",
    "\n",
    "ate = np.zeros((len(qs) - 1, 3))\n",
    "predicted_ate = np.zeros(len(qs) - 1)\n",
    "for it in range(len(qs) - 1):\n",
    "    inds = (qs[it] <= cate_test) & (cate_test < qs[it + 1])\n",
    "    ate[it, :] = (np.mean(dr_test[inds]), np.std(dr_test[inds])/np.sqrt(np.sum(inds)), np.mean(inds))\n",
    "    predicted_ate[it] = np.mean(cate_test[inds])\n",
    "\n",
    "cal = np.sum(ate[:, 2] * np.abs(ate[:, 0] - predicted_ate))\n",
    "calbase = np.sum(ate[:, 2] * np.abs(ate[:, 0] - np.mean(dr_test)))\n",
    "calscore = 1 - cal/calbase\n",
    "plt.title(f'CalScore={calscore:.4f}')\n",
    "plt.errorbar(predicted_ate, ate[:, 0], yerr=1.96*ate[:, 1], fmt='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Based on Uplift Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniform Confidence Band with Multiplier Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on out-of-sample CATE thresholds\n",
    "ugrid = np.linspace(5, 95, 50)\n",
    "qs = np.percentile(overall_best.predict(Xval[:, hetero_inds]), ugrid)\n",
    "\n",
    "ate = np.zeros((len(qs), 3))\n",
    "true_ate = np.zeros(len(qs))\n",
    "psi = np.zeros((len(qs), dr_test.shape[0]))\n",
    "n = len(dr_test)\n",
    "all_ate = np.mean(dr_test)\n",
    "for it in range(len(qs)):\n",
    "    inds = (qs[it] <= cate_test)\n",
    "    prob = np.sum(inds) / n\n",
    "    pate = np.mean(dr_test * inds / prob)\n",
    "    psi[it, :] = (dr_test - all_ate) * (inds / prob - 1)\n",
    "    ate[it, 0] = np.mean(psi[it])\n",
    "    psi[it, :] -= ate[it, 0]\n",
    "    ate[it, 1] = np.sqrt(np.mean(psi[it]**2) / n)\n",
    "    ate[it, 2] = prob\n",
    "    if semi_synth:\n",
    "        true_cate = moment_fn(Xtest, true_f)\n",
    "        true_ate[it] = np.mean((true_cate - np.mean(true_cate)) * (inds * n / np.sum(inds) - 1))\n",
    "\n",
    "if dr_test.shape[0] > 1e6:\n",
    "    mboot = np.zeros((len(qs), 1000))\n",
    "    for it in range(1000):\n",
    "        w = np.random.normal(0, 1, size=(dr_test.shape[0],))\n",
    "        mboot[:, it] = (psi / ate[:, [1]]) @ w / n\n",
    "else:\n",
    "    w = np.random.normal(0, 1, size=(dr_test.shape[0], 1000))\n",
    "    mboot = (psi / ate[:, [1]]) @ w / n\n",
    "max_mboot = np.max(np.abs(mboot), axis=0)\n",
    "max_percentile = np.percentile(max_mboot, 99)\n",
    "print(max_percentile)\n",
    "plt.errorbar(100 - ugrid, ate[:, 0], yerr=max_percentile*ate[:, 1], fmt='o')\n",
    "plt.plot(100 - ugrid, np.zeros(len(ugrid)))\n",
    "if semi_synth:\n",
    "    plt.plot(100 - ugrid, true_ate, 'o')\n",
    "plt.xlabel(\"Percentage treated\")\n",
    "plt.ylabel(\"Gain in Policy Value over treating all\")\n",
    "plt.show()\n",
    "print(f'Heterogeneity Statistic: {np.max(ate[:, 0] - max_percentile*ate[:, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoc_psi = np.sum(psi[:-1] * np.diff(ugrid).reshape(-1, 1) / 100, 0)\n",
    "autoc = np.sum(ate[:-1, 0] * np.diff(ugrid) / 100)\n",
    "autoc_stderr = np.sqrt(np.mean(autoc_psi**2) / n)\n",
    "print(autoc, autoc_stderr, 2.58 * autoc_stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on out-of-sample CATE thresholds\n",
    "ugrid = np.linspace(1, 99, 50)\n",
    "qs = np.percentile(overall_best.predict(Xval[:, hetero_inds]), ugrid)\n",
    "\n",
    "ate = np.zeros((len(qs), 3))\n",
    "true_ate = np.zeros(len(qs))\n",
    "psi = np.zeros((len(qs), dr_test.shape[0]))\n",
    "n = len(dr_test)\n",
    "all_ate = np.mean(dr_test)\n",
    "for it in range(len(qs)):\n",
    "    inds = (qs[it] <= cate_test)\n",
    "    prob = np.mean(inds)\n",
    "    psi[it, :] = (dr_test - all_ate) * (inds - prob)\n",
    "    ate[it, 0] = np.mean(psi[it])\n",
    "    psi[it, :] -= ate[it, 0]\n",
    "    ate[it, 1] = np.sqrt(np.mean(psi[it]**2) / n)\n",
    "    ate[it, 2] = prob\n",
    "    if semi_synth:\n",
    "        true_cate = moment_fn(Xtest, true_f)\n",
    "        true_ate[it] = np.mean((true_cate - np.mean(true_cate)) * (inds - prob))\n",
    "\n",
    "if dr_test.shape[0] > 1e6:\n",
    "    mboot = np.zeros((len(qs), 1000))\n",
    "    for it in range(1000):\n",
    "        w = np.random.normal(0, 1, size=(dr_test.shape[0],))\n",
    "        mboot[:, it] = (psi / ate[:, [1]]) @ w / n\n",
    "else:\n",
    "    w = np.random.normal(0, 1, size=(dr_test.shape[0], 10000))\n",
    "    mboot = (psi / ate[:, [1]]) @ w / n\n",
    "max_mboot = np.max(np.abs(mboot), axis=0)\n",
    "max_percentile = np.percentile(max_mboot, 99)\n",
    "print(max_percentile)\n",
    "plt.errorbar(100 - ugrid, ate[:, 0], yerr=max_percentile*ate[:, 1], fmt='o')\n",
    "plt.plot(100 - ugrid, np.zeros(len(ugrid)))\n",
    "if semi_synth:\n",
    "    plt.plot(100 - ugrid, true_ate, 'o')\n",
    "plt.xlabel(\"Percentage treated\")\n",
    "plt.ylabel(\"Gain in Policy Value over treating all\")\n",
    "plt.show()\n",
    "print(f'Heterogeneity Statistic: {np.max(ate[:, 0] - max_percentile*ate[:, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qini_psi = np.sum(psi[:-1] * np.diff(ugrid).reshape(-1, 1) / 100, 0)\n",
    "qini = np.sum(ate[:-1, 0] * np.diff(ugrid) / 100)\n",
    "qini_stderr = np.sqrt(np.mean(qini_psi**2) / n)\n",
    "print(qini, qini_stderr, 2.58 * qini_stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using In-Sample Quantiles (RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on within sample CATE order. Rank Average Effect. Breaking ties at random\n",
    "ugrid = np.linspace(5, 99, 50)\n",
    "qs = np.percentile(cate_test, ugrid)\n",
    "\n",
    "ate = np.zeros((len(qs), 3))\n",
    "n = len(dr_test)\n",
    "for it in range(len(qs)):\n",
    "    inds = (qs[it] < cate_test)\n",
    "    inds_tie = np.argwhere(qs[it] == cate_test)[:, 0]\n",
    "    if np.sum(inds_tie) > 0:\n",
    "        inds_tie = np.random.choice(inds_tie, int(np.floor((100 - ugrid[it])*n/100 - np.sum(inds))), replace=False)\n",
    "    inds[inds_tie] = True\n",
    "    ate[it, :] = (np.mean(dr_test * inds * n / np.sum(inds) - dr_test),\n",
    "                  np.std(dr_test * inds * n / np.sum(inds) - dr_test) / np.sqrt(n),\n",
    "                  np.sum(inds))\n",
    "\n",
    "plt.errorbar(100 - ugrid, ate[:, 0], yerr=1.96*ate[:, 1], fmt='o')\n",
    "plt.plot(100 - ugrid, np.zeros(len(ugrid)))\n",
    "plt.xlabel(\"Percentage treated\")\n",
    "plt.ylabel(\"Gain in Policy Value over treating all\")\n",
    "plt.show()\n",
    "\n",
    "qini = ate[:, 2] * ate[:, 0]\n",
    "plt.errorbar(100 - ugrid, qini,\n",
    "             yerr=1.96 * ate[:, 1]* ate[:, 2], fmt='o')\n",
    "plt.plot(100 - ugrid, np.zeros(len(ugrid)))\n",
    "plt.xlabel(\"Percentage treated\")\n",
    "plt.ylabel(\"Gain in overall value over treating at random\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklift.metrics import uplift_at_k, uplift_auc_score, qini_auc_score, weighted_average_uplift\n",
    "from sklift.viz import plot_uplift_preds\n",
    "\n",
    "def get_metrics(y_val, uplift_effect, treatment_val):\n",
    "    no_effect_share = round(100 * np.sum(uplift_effect == 0) / len(uplift_effect), 2)\n",
    "    positive_effect_share = round(100 * np.sum(uplift_effect > 0) / len(uplift_effect), 2)\n",
    "    negative_effect_share = round(100 * np.sum(uplift_effect < 0) / len(uplift_effect), 2)\n",
    "    print(f'Model predicts positive effect in visit probability after treatment for {positive_effect_share}% of cases.')\n",
    "    print(f'Model predicts negative effect in visit probability after treatment for {negative_effect_share}% of cases.')\n",
    "    print(f'Model predicts no effect in visit probability after treatment for {no_effect_share}% of cases.')\n",
    "\n",
    "    upliftk = uplift_at_k(\n",
    "        y_true = y_val, \n",
    "        uplift = uplift_effect, \n",
    "        treatment = treatment_val, \n",
    "        strategy='by_group', \n",
    "        k = 0.3\n",
    "    )\n",
    "    \n",
    "    upliftk_all = uplift_at_k(\n",
    "        y_true = y_val, \n",
    "        uplift = uplift_effect, \n",
    "        treatment = treatment_val, \n",
    "        strategy = 'overall',\n",
    "    )\n",
    "\n",
    "    qini_coef = qini_auc_score(\n",
    "        y_true = y_val, \n",
    "        uplift = uplift_effect,\n",
    "        treatment = treatment_val\n",
    "    )\n",
    "\n",
    "    uplift_auc = uplift_auc_score(\n",
    "        y_true = y_val, \n",
    "        uplift = uplift_effect,\n",
    "        treatment = treatment_val\n",
    "    )\n",
    "    wau = weighted_average_uplift(y_true = y_val, uplift = uplift_effect,\n",
    "                                  treatment = treatment_val, strategy = 'by_group')\n",
    "    wau_all = weighted_average_uplift(y_true = y_val, uplift = uplift_effect,\n",
    "                                  treatment = treatment_val, strategy = 'overall')\n",
    "\n",
    "    print(f'uplift at top 30% by group: {upliftk:.3f} by overall: {upliftk_all:.3f}\\n',\n",
    "          f'Weighted average uplift by group: {wau:.3f} by overall: {wau_all:.3f}\\n',\n",
    "          f'AUUC by group: {uplift_auc:.3f}\\n',\n",
    "          f'AUQC by group: {qini_coef:.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklift.viz import plot_qini_curve, plot_uplift_curve, plot_uplift_by_percentile\n",
    "\n",
    "if abtest:\n",
    "    get_metrics(ytest, cate_test, Xtest[:, 0])\n",
    "\n",
    "    qini_disp = plot_qini_curve(\n",
    "        ytest, cate_test, Xtest[:, 0],\n",
    "        perfect=False, name='DREnsemble'\n",
    "    );\n",
    "\n",
    "    qini_disp.figure_.suptitle(\"Qini curve\");\n",
    "    \n",
    "    uplift_disp = plot_uplift_curve(\n",
    "        ytest, cate_test, Xtest[:, 0],\n",
    "        perfect=False, name='DREnsemble'\n",
    "    );\n",
    "\n",
    "    uplift_disp.figure_.suptitle(\"Uplift curve\");\n",
    "    \n",
    "    \n",
    "    plot_uplift_by_percentile(ytest, cate_test, Xtest[:, 0], strategy='overall',\n",
    "                          kind='line', bins=50, string_percentiles=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation based on p-value of linear heterogeneity model in PLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLS(yres_test, np.stack((np.ones(len(tres_test)), tres_test,\n",
    "                        (cate_test - np.mean(cate_test)) * tres_test), axis=-1)).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation based on p-value of DR out of sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLS(dr_test, np.stack((np.ones(len(tres_test)), cate_test - np.mean(cate_test)), axis=-1)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothetical ideal validation for semi-synthetic data\n",
    "\n",
    "OLS(moment_fn(Xtest, true_f),\n",
    "    np.stack((np.ones(len(tres_test)), cate_test - np.mean(cate_test)), axis=-1)).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Intervals with the DRLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.api import OLS\n",
    "Xpoly = PolynomialFeatures(degree=1, include_bias=True).fit_transform(X[:, hetero_inds])\n",
    "lr = OLS(dr_preds, Xpoly).fit()\n",
    "pred_df = lr.get_prediction(Xpoly).summary_frame(alpha=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.argsort(X[:, feat_ind])\n",
    "plt.fill_between(X[inds, feat_ind], pred_df['mean_ci_lower'][inds], pred_df['mean_ci_upper'][inds], alpha=.4)\n",
    "plt.plot(X[inds, feat_ind], pred_df['mean'][inds], alpha=.8)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.argsort(X[:, feat_ind])\n",
    "true_proj = LinearRegression().fit(Xpoly, moment_fn(X, true_f)).predict(Xpoly)\n",
    "inds = np.argsort(true_proj)\n",
    "plt.plot(true_proj[inds], pred_df['mean'][inds])\n",
    "plt.fill_between(true_proj[inds], pred_df['mean_ci_lower'][inds], pred_df['mean_ci_upper'][inds], alpha=.4)\n",
    "plt.plot(np.linspace(np.min(true_proj), np.max(true_proj), 100),\n",
    "         np.linspace(np.min(true_proj), np.max(true_proj), 100))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "df = pd.DataFrame(X[:, hetero_inds], columns=np.array(column_names)[hetero_inds])\n",
    "df['dr'] = dr_preds\n",
    "lr = ols('dr ~ C(polviews)', df).fit()\n",
    "lr.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = lr.get_prediction(df).summary_frame(alpha=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.argsort(df['polviews']) #X[:, feat_ind])\n",
    "plt.plot(df['polviews'].iloc[inds], pred_df['mean'][inds])\n",
    "plt.fill_between(df['polviews'].iloc[inds], pred_df['mean_ci_lower'][inds], pred_df['mean_ci_upper'][inds], alpha=.4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_proj = LinearRegression().fit(np.log(df[['inc']]), moment_fn(X, true_f)).predict(np.log(df[['inc']]))\n",
    "inds = np.argsort(true_proj)\n",
    "plt.plot(true_proj[inds], pred_df['mean'][inds])\n",
    "plt.fill_between(true_proj[inds], pred_df['mean_ci_lower'][inds], pred_df['mean_ci_upper'][inds], alpha=.4)\n",
    "plt.plot(np.linspace(np.min(true_proj), np.max(true_proj), 100),\n",
    "         np.linspace(np.min(true_proj), np.max(true_proj), 100))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Parametric Confidence Intervals with Causal Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from econml.grf import CausalForest\n",
    "\n",
    "est = CausalForest(4000, max_depth=5, max_samples=.4, min_samples_leaf=50, min_weight_fraction_leaf=.0)\n",
    "est.fit(X[:, hetero_inds], tres, yres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feat = hetero_inds[np.argsort(est.feature_importances_)[-1]]\n",
    "print(column_names[top_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.argsort(df['polviews']) #X[:, feat_ind])\n",
    "preds, lb, ub = est.predict(X[:, hetero_inds], interval=True, alpha=.01)\n",
    "plt.plot(df['polviews'].iloc[inds], preds[inds])\n",
    "plt.fill_between(df['polviews'].iloc[inds], lb[inds].flatten(), ub[inds].flatten(), alpha=.4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_proj = moment_fn(X, true_f)\n",
    "inds = np.argsort(true_proj)\n",
    "plt.plot(true_proj[inds], preds[inds])\n",
    "plt.fill_between(true_proj[inds], lb[inds].flatten(), ub[inds].flatten(), alpha=.4)\n",
    "plt.plot(np.linspace(np.min(true_proj), np.max(true_proj), 100),\n",
    "         np.linspace(np.min(true_proj), np.max(true_proj), 100))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from econml.dml import CausalForestDML\n",
    "\n",
    "est = CausalForestDML(model_y=model_y(), model_t=model_t(),\n",
    "                      n_estimators=4000, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.tune(y, X[:, 0], X=X[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.fit(y, X[:, 0], X=X[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X[:, 1:], columns=np.array(column_names)[1:])\n",
    "grid = np.unique(np.percentile(df['polviews'], np.arange(0, 110, 20)))\n",
    "dfpd = pd.DataFrame(np.tile(np.median(df, axis=0, keepdims=True), (len(grid), 1)),\n",
    "                    columns=df.columns)\n",
    "dfpd['polviews'] = grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf = est.effect_inference(dfpd)\n",
    "preds, lb, ub = inf.pred, *inf.conf_int(alpha=0.01)\n",
    "plt.errorbar(dfpd['polviews'], preds, yerr=(preds-lb, ub-preds))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Parametric Confidence Intervals with Doubly Robust Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from econml.grf import RegressionForest\n",
    "\n",
    "est = RegressionForest(4000, max_depth=5, max_samples=.4, min_samples_leaf=50,\n",
    "                       min_weight_fraction_leaf=.0, n_jobs=1, verbose=3)\n",
    "est.fit(X[:, hetero_inds], dr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feat = hetero_inds[np.argsort(est.feature_importances_)[-1]]\n",
    "print(column_names[top_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X[:, 1:], columns=np.array(column_names)[1:])\n",
    "grid = np.unique(np.percentile(df[column_names[top_feat]], np.arange(0, 110, 20)))\n",
    "dfpd = pd.DataFrame(np.tile(np.median(df, axis=0, keepdims=True), (len(grid), 1)),\n",
    "                    columns=df.columns)\n",
    "dfpd['polviews'] = grid\n",
    "\n",
    "preds, lb, ub = est.predict(dfpd, interval=True, alpha=.01)\n",
    "plt.errorbar(dfpd['polviews'], preds, yerr=(preds-lb, ub-preds), alpha=.4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_proj = moment_fn(Xs, true_f)\n",
    "inds = np.argsort(true_proj)\n",
    "plt.plot(true_proj[inds], preds[inds])\n",
    "plt.fill_between(true_proj[inds], lb[inds].flatten(), ub[inds].flatten(), alpha=.4)\n",
    "plt.plot(np.linspace(np.min(true_proj), np.max(true_proj), 100),\n",
    "         np.linspace(np.min(true_proj), np.max(true_proj), 100))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
